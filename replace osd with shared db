Замена диска под OSD c выделенным DB/WAL в оркестрируемом ceph

Замена OSD при выделенном db_device

Пример - на кластере s3.cluster.name требуется заменить osd.18, который расположен на
ноде s3-06 и включает в себя два устройства:
для данных - /dev/dm-4 (/dev/mapper/ceph--065f02d1--0ce9--4d67--8472--132881e75365-osd--block--abddb3d6--9245--4c44--90da--e452788d44e2)
для базы - /dev/dm-13 (/dev/mapper/ceph--4e507818--90df--4c53--8321--00bf18b48896-osd--db--448112cd--09b6--45e9--8bee–013e1ef0f4ca)

1. Зайти на хост, где будет производиться замена диска
ssh s3-06.node.name

2. Получить информацию о томах под OSD
Вариант "А"
Вызываем команду:
ls -lah /var/lib/ceph/b12308900-55yh-22ca-a256-b66gb4879765/osd.18/ | grep block.db
получаем
lrwxrwxrwx 1 167 167 108 Apr 20 13:46 block.db -> /dev/mapper/ceph--4e507818--90df--4c53--8321--00bf18b48896-osd--db--448112cd--09b6--45e9--8bee--013e1ef0f4ca

Вариант "Б"
Вызываем команду:
ceph osd metadata 18

где нас интересуют поля и их значения:
"bluefs_db_dev_node": "/dev/dm-13"
"bluestore_bdev_dev_node": "/dev/dm-4"

используя эту информацию, находим идентификатор тома с базой:
ls -lah /dev/mapper/ | grep dm-13
получаем
lrwxrwxrwx 1 root root 8 Apr 20 13:52 ceph--4e507818--90df--4c53--8321--00bf18b48896-osd--db--448112cd--09b6--45e9--8bee--013e1ef0f4ca -> ../dm-13

3. В идентификаторе тома - ceph--4e507818--90df--4c53--8321--00bf18b48896-osd--db--448112cd--09b6--45e9--8bee–013e1ef0f4ca - содержится информация о VG и LV, где
VG: ceph-4e507818-90df-4c53-8321-00bf18b48896
LV: osd-db-448112cd-09b6-45e9-8bee-013e1ef0f4ca
Обращаем внимание на дефисы (разделители)

Убедимся в этом
lvs | grep osd-db-448112cd-09b6-45e9-8bee-013e1ef0f4ca
получаем
osd-db-448112cd-09b6-45e9-8bee-013e1ef0f4ca ceph-4e507818-90df-4c53-8321-00bf18b48896 -wi-ao---- <745.21g

4. Получаем информацию о привязке OSD к провижн-группе
cat /var/lib/ceph/b12308900-55yh-22ca-a256-b66gb4879765/osd.18/unit.meta | grep service
получаем
"service_name": "osd.s3-06_hdd03",

5. Далее переходим к выводу OSD и замене диска
ceph osd out 18

6. Дожидаемся окончания миграции pg, затем удаляем OSD
ceph orch osd rm 18 --replace

7. Меняем диск на хосте, НЕ переводя его в HBA-режим (не "прокидывая" в ОС)

8. Ставим на паузу оркестратор, чтобы провижинер OSD не создал новый OSD с
новой базой на одном и том же заменённом диске
ceph orch pause

9. Переводим диск в HBA-режим ("прокидываем" в ОС)

10. Создаём OSD, используя информацию о томе с базой, полученную ранее
ceph orch daemon add osd s3-06:data_devices=/dev/sdb,db_devices=ceph-4e507818-90df-4c53-8321-00bf18b48896/osd-db-448112cd-09b6-45e9-8bee-013e1ef0f4ca
где:
s3-06 - имя ноды, где меняется диск
data_devices=/dev/sdb - заменённый диск под данные
db_devices=ceph-4e507818-90df-4c53-8321-00bf18b48896/osd-db-448112cd-09b6-45e9-8bee-013e1ef0f4ca - существующий том с базой (формат db_devices=VG/LV)

Если OSD не создаётся, указывая причину в том, что он уже создан, удаляем
каталог с ним
rm -rf /var/lib/ceph/b12308900-55yh-22ca-a256-b66gb4879765/osd.18/
и повторяем попытку создать

11. Снимаем с паузы оркестратор
ceph orch resume

12. Дожидаемся пока OSD перейдёт в статус IN+UP, затем корректируем
привязку поднятого OSD к провижн группе:
в файле приводим значение поля "service_name" к изначальному
nano /var/lib/ceph/b12308900-55yh-22ca-a256-b66gb4879765/osd.18/unit.meta
"service_name": "osd.s3-06_hdd03"
Сохраняем файл, дожидаемся пока оркестратор перечитает конфигурацию

13. Во встроенном дашборде проверяем состав провижн-группы osd.s3-06_hdd03 в
разделе cluster > services и привязку вновь поднятого OSD к старому журналу
